{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "# 1. Models- Input and Outputs (Model IO)"
      ],
      "metadata": {
        "id": "PskFNj1e_84W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MqCwkwRF_9_V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKyx0k-Z_5PK",
        "outputId": "5920979c-4a1f-45c1-8d9f-240480880033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.9)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.10 (from langchain-community)\n",
            "  Downloading langchain-0.3.10-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.22 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.22-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.10->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.10->langchain-community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.22->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.28.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.22->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.10->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.10->langchain-community) (2.27.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.10-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.10-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.22-py3-none-any.whl (409 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.7/409.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.9\n",
            "    Uninstalling langchain-0.3.9:\n",
            "      Successfully uninstalled langchain-0.3.9\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-0.3.10 langchain-community-0.3.10 langchain-core-0.3.22 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community\n",
        "\n",
        "%pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HUGGINGFACEHUB_API_TOKEN= userdata.get('HUGGING_FACE_API_KEY')"
      ],
      "metadata": {
        "id": "lZsDFGZEBDzC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "Pd79Ih0pAMJO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token = HUGGINGFACEHUB_API_TOKEN)"
      ],
      "metadata": {
        "id": "SWUrs6YhAX-s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import transformers, torch\n",
        "# from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "# model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# pipe = transformers.pipeline(\n",
        "#     \"text-generation\",\n",
        "#     model=model_id,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     device_map=\"auto\",\n",
        "#     max_new_tokens=2048,\n",
        "#     do_sample=False,\n",
        "#     repetition_penalty=1.03\n",
        "# )\n",
        "# pipe.tokenizer.pad_token_id = pipe.model.config.eos_token_id\n",
        "\n",
        "\n",
        "\n",
        "# model_kwargs = {'temperature':0.3}\n",
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = model_kwargs)\n",
        "\n",
        "import transformers, torch\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "pipe = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens=2048,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03\n",
        ")\n",
        "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token  # Set pad_token to eos_token\n",
        "\n",
        "model_kwargs = {'temperature':0.3}\n",
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = model_kwargs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgNHUSeuAYCK",
        "outputId": "3ab51cfa-c2ca-43ef-ce2d-75559c7a3cfe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Prompt Templates"
      ],
      "metadata": {
        "id": "4XShjfodAkiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "ISeZaPnDAYFa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM\n",
        "llm(\"What is Data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "CeS3SaAcAYJA",
        "outputId": "a63a2f69-3b0b-4e1c-ee7f-7558984960d6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-5f6bdbcab589>:2: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  llm(\"What is Data\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is Data Analytics?\\nData analytics is the process of examining and interpreting data to gain insights, identify trends, and make informed decisions. It involves using various techniques, tools, and technologies to extract meaningful information from large datasets and turn it into actionable knowledge.\\n\\nData analytics involves several key steps:\\n\\n1. **Data Collection**: Gathering data from various sources, such as databases, sensors, or user input.\\n2. **Data Cleaning**: Ensuring the quality and accuracy of the data by removing errors, inconsistencies, and duplicates.\\n3. **Data Transformation**: Converting the data into a format that can be easily analyzed.\\n4. **Data Analysis**: Using statistical and mathematical techniques to identify patterns, trends, and correlations in the data.\\n5. **Insight Generation**: Interpreting the results of the analysis to gain insights and make informed decisions.\\n6. **Communication**: Presenting the findings to stakeholders through reports, dashboards, or other visualizations.\\n\\nData analytics is used in various industries, including:\\n\\n1. **Business**: To optimize operations, improve customer service, and increase revenue.\\n2. **Healthcare**: To diagnose diseases, predict patient outcomes, and develop personalized treatment plans.\\n3. **Finance**: To detect fraud, manage risk, and optimize investment portfolios.\\n4. **Marketing**: To understand customer behavior, optimize advertising campaigns, and personalize product recommendations.\\n5. **Government**: To analyze crime patterns, track population growth, and inform policy decisions.\\n\\nSome common data analytics tools and techniques include:\\n\\n1. **Spreadsheets**: Microsoft Excel, Google Sheets\\n2. **Statistical software**: R, Python, SAS\\n3. **Machine learning algorithms**: Neural networks, decision trees, clustering\\n4. **Data visualization tools**: Tableau, Power BI, D3.js\\n5. **Big data technologies**: Hadoop, Spark, NoSQL databases\\n\\nBy applying data analytics techniques and tools, organizations can gain valuable insights, make informed decisions, and drive business success.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "planet= \"Earth\"\n",
        "print(llm(f\"Here is a fact about {planet}\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePgAdGjOCj3d",
        "outputId": "c1c6faf5-a09c-443b-b3a1-0f9e30259849"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here is a fact about Earth's atmosphere:\n",
            "\n",
            "The Earth's atmosphere is composed of approximately 78% nitrogen, 21% oxygen, and 1% other gases.\n",
            "\n",
            "This information is based on data from the National Oceanic and Atmospheric Administration (NOAA) and the National Aeronautics and Space Administration (NASA).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate"
      ],
      "metadata": {
        "id": "8ryLf4meC0M8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_input_prompt = PromptTemplate(input_variables=[],\n",
        "                                 template=\"Tell me a fact\")"
      ],
      "metadata": {
        "id": "m3Yud3noDDWX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_input_prompt.format()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8gFEmD6-DRRf",
        "outputId": "6369090e-bde3-411a-cb6a-47ee538c6760"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a fact'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm(no_input_prompt.format())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "s_0LzWXjDUC0",
        "outputId": "86310374-787d-46fc-da1c-bad50321a4a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tell me a fact about the world's largest living organism.\\nThe world's largest living organism is a fungus called Armillaria ostoyae, also known as the honey mushroom. It covers over 2,200 acres in Oregon, USA, and is estimated to be around 2,400 years old!\\n\\nWould you like to know more about this incredible fungus?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "single_input_prompt = PromptTemplate(input_variables=[\"topic\"],\n",
        "                                    template=\"Tell me a fact about {topic}\")"
      ],
      "metadata": {
        "id": "9mKWpnuYDYLP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(single_input_prompt.format(topic=\"Mars\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "azC1wjRXDlEj",
        "outputId": "7c7a7e5e-e48d-41d1-9886-a4d8eaaa6345"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a fact about Mars\\nMars is the only planet in our solar system that rotates in the opposite direction of its orbit around the Sun. This phenomenon is known as retrograde rotation.\\n\\nWould you like to know more about Mars?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multi_input_prompt = PromptTemplate(input_variables=[\"topic\", \"level\"],\n",
        "                                    template=\"Tell me a fact about {topic} for a {level} student\")"
      ],
      "metadata": {
        "id": "M4hoJbwODqJa"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm(multi_input_prompt.format(topic=\"the ocean\", level=\"3rd grade\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "4cuoipCGEHh5",
        "outputId": "0a752a2a-9b7f-46e8-bdbc-7358fdae73cb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tell me a fact about the ocean for a 3rd grade student.\\n\\nHere's a fun fact:\\n\\nThe deepest part of the ocean is called the Mariana Trench. It's so deep that it's harder to imagine than Mount Everest, which is over 29,000 feet tall! Can you imagine being that deep down in the ocean?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n",
        "                               SystemMessagePromptTemplate, AIMessagePromptTemplate,\n",
        "                               PromptTemplate)\n",
        "\n",
        "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)"
      ],
      "metadata": {
        "id": "suysBEUnEP93"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"You are an AI recipe assistant that specializes in {dietary_preference} dishes that can be prepared in {cooking_time}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "# print(system_message_prompt)"
      ],
      "metadata": {
        "id": "JW9qj2udJdul"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template = \"{recipe_request}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "# print(human_message_prompt)"
      ],
      "metadata": {
        "id": "tIKwyMJwKkOj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqbkEeHK0MP",
        "outputId": "0aefaeac-2217-4103-b807-8ee51a225ee8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cooking_time', 'dietary_preference']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_message_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UrGT_EYK4sw",
        "outputId": "b3e4ab78-90e3-4acf-cf05-fd33345e0c6e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['recipe_request']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "chat_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJzSnwmlK7Ez",
        "outputId": "1552342e-b6fc-484c-bbb4-d7a805826dc7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cooking_time', 'dietary_preference', 'recipe_request']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt.format_messages(cooking_time= \"60 min\",\n",
        "                            dietary_preference= \"Quick Snack\",\n",
        "                            recipe_request= \"Vega\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umgi5U_mLLer",
        "outputId": "920ce877-4251-4c8b-d38b-ce01c3589fff"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are an AI recipe assistant that specializes in Quick Snack dishes that can be prepared in 60 min.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Vega', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= chat_prompt.format_prompt(cooking_time= \"60 min\",\n",
        "                            dietary_preference= \"Quick Snack\",\n",
        "                            recipe_request= \"Vega\").to_messages()"
      ],
      "metadata": {
        "id": "s7QpBxPILoL1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result= chat(prompt)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m16ArwIsLz9q",
        "outputId": "fe516db5-1505-4abc-a497-7e5c0b02cbbd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-9a026a1f9636>:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result= chat(prompt)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "You are an AI recipe assistant that specializes in Quick Snack dishes that can be prepared in 60 min.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Vega<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here are some quick and delicious vegan snack ideas that can be prepared in under 60 minutes:\n",
            "\n",
            "1. **Spicy Roasted Chickpeas**: Rinse 1 can of chickpeas, dry them, and roast in the oven at 400°F (200°C) for 20-25 minutes, shaking halfway through. Season with salt, pepper, and your favorite spices.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 30 minutes | Total Time: 40 minutes\n",
            "\n",
            "2. **Avocado Toast**: Toast whole grain bread, mash 1 ripe avocado, and spread on top. Add salt, pepper, and a squeeze of lemon juice. You can also add some cherry tomatoes or sliced red onion for extra flavor.\n",
            "\n",
            "Prep Time: 5 minutes | Cook Time: 0 minutes | Total Time: 5 minutes\n",
            "\n",
            "3. **Quinoa Salad**: Cook quinoa according to package instructions. Mix with chopped veggies like bell peppers, cucumber, and cherry tomatoes. Add a simple vinaigrette made from lemon juice, olive oil, and your favorite herbs.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 15 minutes | Total Time: 25 minutes\n",
            "\n",
            "4. **Vegan Energy Balls**: Mix together 1/2 cup rolled oats, 1/2 cup nut butter, 1/4 cup honey, and 1/4 cup chopped nuts. Roll into small balls and refrigerate for at least 10 minutes.\n",
            "\n",
            "Prep Time: 5 minutes | Cook Time: 0 minutes | Total Time: 5 minutes\n",
            "\n",
            "5. **Lentil Soup**: Saute onions, garlic, and carrots in a pot until softened. Add lentils, vegetable broth, and diced tomatoes. Simmer for 20-25 minutes, then season with your favorite spices.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 25 minutes | Total Time: 35 minutes\n",
            "\n",
            "6. **Vegan Hummus**: Blend together chickpeas, lemon juice, tahini, garlic, and olive oil. Serve with carrot sticks or pita chips.\n",
            "\n",
            "Prep Time: 5 minutes | Cook Time: 0 minutes | Total Time: 5 minutes\n",
            "\n",
            "7. **Grilled Veggie Wrap**: Fill a whole wheat wrap with roasted veggies like zucchini, bell peppers, and hummus. Add some sliced avocado and sprouts for extra creaminess.\n",
            "\n",
            "Prep Time: 5 minutes | Cook Time: 10 minutes | Total Time: 15 minutes\n",
            "\n",
            "8. **Baked Sweet Potato Fries**: Slice sweet potatoes into thin strips, toss with oil and your favorite seasonings. Bake in the oven at 400°F (200°C) for 20-25 minutes, flipping halfway through.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 20 minutes | Total Time: 30 minutes\n",
            "\n",
            "9. **Quiche Lorraine**: Whisk together eggs, milk, and diced veggies. Pour into a pie crust and bake at 375°F (190°C) for 25-30 minutes.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 25 minutes | Total Time: 35 minutes\n",
            "\n",
            "10. **Roasted Cauliflower Bites**: Toss cauliflower florets with oil, salt, and your favorite seasonings. Roast in the oven at 400°F (200°C) for 20-25 minutes, flipping halfway through.\n",
            "\n",
            "Prep Time: 10 minutes | Cook Time: 20 minutes | Total Time: 30 minutes\n",
            "\n",
            "Which one of these quick and delicious vegan snacks will you try first?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Prompt and Models Exercise\n",
        "\n",
        "### TASK: Create a Python function that uses Prompts and Chat internally to given travel ideas related to two variables:\n",
        "\n",
        "* An Interest or Hobby\n",
        "* A Budget\n",
        "\n",
        "Remember that you should also decide on a system prompt. The end function will just be a nice wrapper on top of all the previous LangChain components we've discussed earlier.\n",
        "\n",
        "```python\n",
        "def travel_idea(interest, budget):\n",
        "  \"\"\"\n",
        "  INPUTS:\n",
        "      interest: A str interest or hobby (e.g. fishing)\n",
        "      budget: A str budget (e.g. $10,00)\n",
        "  \"\"\"\n",
        "  pass\n",
        "```\n",
        "\n",
        "#### Solution"
      ],
      "metadata": {
        "id": "Yj2mabiiNt5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "\n",
        "from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n",
        "                               SystemMessagePromptTemplate, AIMessagePromptTemplate,\n",
        "                               PromptTemplate)\n",
        "\n",
        "from langchain.schema import (AIMessage, HumanMessage, SystemMessage)"
      ],
      "metadata": {
        "id": "9ClKSnhzPv0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def travel_idea(interest, budget):\n",
        "  \"\"\"\n",
        "  INPUTS:\n",
        "      interest: A str interest or hobby (e.g. fishing)\n",
        "      budget: A str budget (e.g. $10,00)\n",
        "  \"\"\"\n",
        "  # DEFINE SYSTEM interest, budget template-->PromptTemplate\n",
        "  system_template= \"You are a travel agent that helpd people plan trips about {interest} on a budget of {budget}.\"\n",
        "  system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "  # DEFINE HUMAN interest, budget template-->PromptTemplate\n",
        "  human_template= \"Please tell me an example itinerary\"\n",
        "  human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "  # Compile ChatPromptTemplate\n",
        "  chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "  # Insert Variables-->ChatPromptTemplate\n",
        "  request= chat_prompt.format_prompt(interest=interest, budget=budget).to_messages()\n",
        "\n",
        "  # Chat Request\n",
        "  chat = ChatHuggingFace(llm=llm)\n",
        "  result= chat(request)\n",
        "\n",
        "  return result.content"
      ],
      "metadata": {
        "id": "C1HwVt1jMIuB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(travel_idea(interest=\"fishing\", budget= \"$10,000\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGsUWXbNNypo",
        "outputId": "26e90a52-ac9a-428b-966b-2189bfd16933"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "You are a travel agent that helpd people plan trips about fishing on a budget of $10,000.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Please tell me an example itinerary<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "I'd be happy to help you plan a fishing trip on a budget of $10,000. Here's an example itinerary for a 7-day fishing trip to the Pacific Northwest:\n",
            "\n",
            "**Destination:** The Columbia River Gorge, Oregon\n",
            "\n",
            "**Duration:** 7 days, 6 nights\n",
            "\n",
            "**Budget:** $9,500\n",
            "\n",
            "**Accommodation:** Camping (approx. $1,200)\n",
            "\n",
            "**Transportation:** Flights from Los Angeles to Portland, OR (approx. $800)\n",
            "\n",
            "**Fishing Gear:**\n",
            "\n",
            "* Rods and reels: $1,500\n",
            "* Lures and bait: $500\n",
            "* Fishing licenses: $200\n",
            "* Tackle box and tackle: $300\n",
            "* Miscellaneous gear (e.g., net, lines, etc.): $200\n",
            "\n",
            "**Food and Drink:**\n",
            "\n",
            "* Groceries for meals: $800\n",
            "* Snacks and incidentals: $200\n",
            "* Total food budget: $1,000\n",
            "\n",
            "**Other Expenses:**\n",
            "\n",
            "* Guided fishing tour (optional): $2,000\n",
            "* Transportation to and from the river: $200\n",
            "* Miscellaneous expenses (e.g., souvenirs, etc.): $500\n",
            "\n",
            "**Total Budget:** $9,500\n",
            "\n",
            "**Itinerary:**\n",
            "\n",
            "Day 1: Arrival in Portland, OR\n",
            "\n",
            "* Arrive at Portland International Airport and pick up rental car\n",
            "* Drive to the Columbia River Gorge (approx. 2 hours)\n",
            "* Check-in at a budget-friendly hotel in Hood River, OR (approx. $100 per night)\n",
            "* Spend the day exploring the town and getting settled\n",
            "\n",
            "Day 2: Fishing Day 1\n",
            "\n",
            "* Drive to the Columbia River (approx. 30 minutes)\n",
            "* Launch a guided fishing tour on the river (approx. $200 per person)\n",
            "* Spend the day fishing for salmon, steelhead, and trout\n",
            "* Return to Hood River in the evening\n",
            "\n",
            "Day 3: Fishing Day 2\n",
            "\n",
            "* Drive to the Columbia River (approx. 1 hour)\n",
            "* Launch another guided fishing tour (approx. $200 per person)\n",
            "* Spend the day fishing for salmon, steelhead, and trout\n",
            "* Return to Hood River in the evening\n",
            "\n",
            "Day 4: Fishing Day 3\n",
            "\n",
            "* Drive to the Columbia River (approx. 1 hour)\n",
            "* Launch a solo fishing trip (approx. $100 per day)\n",
            "* Spend the day fishing for salmon, steelhead, and trout\n",
            "* Return to Hood River in the evening\n",
            "\n",
            "Day 5: Fishing Day 4\n",
            "\n",
            "* Drive to the Columbia River (approx. 1 hour)\n",
            "* Launch a solo fishing trip (approx. $100 per day)\n",
            "* Spend the day fishing for salmon, steelhead, and trout\n",
            "* Return to Hood River in the evening\n",
            "\n",
            "Day 6: Fishing Day 5\n",
            "\n",
            "* Drive to the Columbia River (approx. 1 hour)\n",
            "* Launch a solo fishing trip (approx. $100 per day)\n",
            "* Spend the day fishing for salmon, steelhead, and trout\n",
            "* Return to Hood River in the evening\n",
            "\n",
            "Day 7: Departure\n",
            "\n",
            "* Spend the morning shopping for souvenirs and packing up the car\n",
            "* Depart for Portland International Airport and fly back to Los Angeles (approx. $800)\n",
            "\n",
            "**Total Cost:** $9,500\n",
            "\n",
            "This itinerary provides a mix of guided fishing tours and solo fishing trips, with a focus on catching a variety of species. The budget breakdown is as follows:\n",
            "\n",
            "* Accommodation: $1,200\n",
            "* Transportation: $800\n",
            "* Fishing gear: $1,500\n",
            "* Food and drink: $1,000\n",
            "* Guided fishing tours: $2,000\n",
            "* Miscellaneous expenses: $500\n",
            "\n",
            "This itinerary should provide a great balance of fishing and relaxation, while staying within your budget of $10,000. However, please note that prices may vary depending on the time of year and availability.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Few Shot Prompt Templates"
      ],
      "metadata": {
        "id": "r3e1SSGMTEwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (ChatPromptTemplate, HumanMessagePromptTemplate,\n",
        "                               SystemMessagePromptTemplate, AIMessagePromptTemplate,\n",
        "                               PromptTemplate)"
      ],
      "metadata": {
        "id": "UJcU1WOHS53s"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AI BOT LEGAL --> Simple Terms"
      ],
      "metadata": {
        "id": "DoFCK9hmUJNo"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_template = \"You are a helpful legal assistant that translates complext legal terms into plain and understandable terms.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "print(system_message_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0Zv4KB7UZH8",
        "outputId": "9825836f-39bb-40ba-8e4d-d5a667365296"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful legal assistant that translates complext legal terms into plain and understandable terms.') additional_kwargs={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FEW SHOT\n",
        "# INPUT HUMAN\n",
        "# OUTPUT AI\n",
        "\n",
        "legal_text= \"\"\"\n",
        "The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, or\n",
        "unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall\n",
        "remain in full force and effect to the maximum extent permitted by law.\n",
        "\"\"\"\n",
        "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
        "\n",
        "plain_text= \"The rules in this agreement can be separated.\"\n",
        "example_output_one= AIMessagePromptTemplate.from_template(plain_text)"
      ],
      "metadata": {
        "id": "gN6r0h__U2I9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_template= \"{legal_text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n"
      ],
      "metadata": {
        "id": "Y5TugjEvVyIW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt= ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, example_input_one, example_output_one, human_message_prompt])\n",
        "chat_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOqcBXFbW2ge",
        "outputId": "3301b16e-b66e-4c19-c4cb-d0e4f9b1fe69"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['legal_text']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_legal_text = \"\"\"This grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee,\n",
        "his heirs and assigns, all of the grantor's rights, title, and interest in and to the said property, subject to all existing encumbrances,\n",
        "liens, and easements, as recorded in the official records of the country, and any applicable convenants, conditions, and restrictions\n",
        "affecting the property, in consideration of the sum of [purchase price] paid by the grantee.\"\"\"\n",
        "\n",
        "request= chat_prompt.format_prompt(legal_text= example_legal_text).to_messages()\n",
        "request"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwKPR5OvXIav",
        "outputId": "b64ffc1b-e234-44c4-82fe-66cfd58efd82"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful legal assistant that translates complext legal terms into plain and understandable terms.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='\\nThe provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, or \\nunenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall \\nremain in full force and effect to the maximum extent permitted by law.\\n', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The rules in this agreement can be separated.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"This grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee,\\nhis heirs and assigns, all of the grantor's rights, title, and interest in and to the said property, subject to all existing encumbrances,\\nliens, and easements, as recorded in the official records of the country, and any applicable convenants, conditions, and restrictions \\naffecting the property, in consideration of the sum of [purchase price] paid by the grantee.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result1= chat(request)\n",
        "print(result1.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPqdUGO6ad9R",
        "outputId": "80d23403-9252-4b08-f025-d04b4b8434d1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "You are a helpful legal assistant that translates complext legal terms into plain and understandable terms.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "The provisions herein shall be severable, and if any provision or portion thereof is deemed invalid, or \n",
            "unenforceable by a court of competent jurisdiction, the remaining provisions or portions thereof shall \n",
            "remain in full force and effect to the maximum extent permitted by law.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "The rules in this agreement can be separated.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "This grantor, being the fee simple owner of the real property herein described, conveys and warrants to the grantee,\n",
            "his heirs and assigns, all of the grantor's rights, title, and interest in and to the said property, subject to all existing encumbrances,\n",
            "liens, and easements, as recorded in the official records of the country, and any applicable convenants, conditions, and restrictions \n",
            "affecting the property, in consideration of the sum of [purchase price] paid by the grantee.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "This is a deed of conveyance, which is a document that transfers ownership of real property from one party (the grantor) to another (the grantee). \n",
            "\n",
            "Here's a breakdown of what it says:\n",
            "\n",
            "- The grantor is transferring their entire interest in the property to the grantee.\n",
            "- The grantor is making this transfer subject to certain conditions, including:\n",
            "  - Any existing encumbrances, liens, or easements on the property.\n",
            "  - Any applicable covenants, conditions, or restrictions that affect the property.\n",
            "- The grantor is also paying a purchase price for the property.\n",
            "\n",
            "Note that this is a standard form of deed, which is a formal document used to transfer ownership of real property.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Parsing Output- Part One"
      ],
      "metadata": {
        "id": "ODxnrI4QbLUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "4A9bHhZJata6"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step1: Import Parser\n",
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser= CommaSeparatedListOutputParser()"
      ],
      "metadata": {
        "id": "6PzcO7rQbSzl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step2: Format Instructions\n",
        "output_parser.get_format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s9CAa4K7cERm",
        "outputId": "9f7993e8-165a-422a-e84f-04a48dc4917f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reply= \"red, blue, green\"\n",
        "print(list(reply))\n",
        "print(reply.split(\",\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-gIPr7fcO13",
        "outputId": "208c21be-d872-4158-8c42-e9a61e62f8e5"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['r', 'e', 'd', ',', ' ', 'b', 'l', 'u', 'e', ',', ' ', 'g', 'r', 'e', 'e', 'n']\n",
            "['red', ' blue', ' green']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(reply)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Kb1td6c7bl",
        "outputId": "37138977-7658-4e56-afd6-ffa1e5e2b888"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['red', 'blue', 'green']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_template= \"{request}\\n{format_instructions}\"\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(human_template)"
      ],
      "metadata": {
        "id": "aur9A6GHdDPy"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt= ChatPromptTemplate.from_messages([human_prompt])\n",
        "\n",
        "chat_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrA_rVI2dUpV",
        "outputId": "5da4266c-d52e-4208-8a48-2b283fd7ca4a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['format_instructions', 'request']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_request= chat_prompt.format_prompt(request= \"Give me 5 characteristics of dogs\",\n",
        "                                         format_instructions= output_parser.get_format_instructions()).to_messages()"
      ],
      "metadata": {
        "id": "HLYr_-f3dsaW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result= model(model_request)\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "PlubmEcIeSTT",
        "outputId": "59404ed3-8052-492d-a98a-e3df5b35b871"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nGive me 5 characteristics of dogs\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere are 5 characteristics of dogs:\\n\\nLoyal, intelligent, energetic, social, affectionate'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhGHhJG8eg-W",
        "outputId": "1fd2541f-2d35-4fa8-81d4-4af79e706518"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Give me 5 characteristics of dogs\n",
            "Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here are 5 characteristics of dogs:\n",
            "\n",
            "Loyal, intelligent, energetic, social, affectionate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1qSc35zeo97",
        "outputId": "54c35f27-183e-48ee-b949-62edd4b60da2"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>',\n",
              " 'Cutting Knowledge Date: December 2023',\n",
              " 'Today Date: 07 Dec 2024',\n",
              " '<|eot_id|><|start_header_id|>user<|end_header_id|>',\n",
              " 'Give me 5 characteristics of dogs',\n",
              " 'Your response should be a list of comma separated values',\n",
              " 'eg: `foo',\n",
              " 'bar',\n",
              " 'baz` or `foo',\n",
              " 'bar',\n",
              " 'baz`<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              " 'Here are 5 characteristics of dogs:',\n",
              " 'Loyal',\n",
              " 'intelligent',\n",
              " 'energetic',\n",
              " 'social',\n",
              " 'affectionate']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_request= chat_prompt.format_prompt(request= \"Write a blog post about dogs\",\n",
        "                                         format_instructions= output_parser.get_format_instructions()).to_messages()\n",
        "\n",
        "result= model(model_request)\n",
        "output_parser.parse(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcbO3brxevDn",
        "outputId": "607dffc8-c977-404a-988b-00fc4ca8521e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>',\n",
              " 'Cutting Knowledge Date: December 2023',\n",
              " 'Today Date: 07 Dec 2024',\n",
              " '<|eot_id|><|start_header_id|>user<|end_header_id|>',\n",
              " 'Write a blog post about dogs',\n",
              " 'Your response should be a list of comma separated values',\n",
              " 'eg: `foo',\n",
              " 'bar',\n",
              " 'baz` or `foo',\n",
              " 'bar',\n",
              " 'baz`<|eot_id|><|start_header_id|>assistant<|end_header_id|>',\n",
              " '**The Joy of Dogs: A Comprehensive Guide**',\n",
              " 'Dogs are one of the most beloved pets in the world',\n",
              " 'and for good reason. They bring immense joy',\n",
              " 'companionship',\n",
              " \"and love to our lives. Here's a list of some interesting facts about dogs:\",\n",
              " 'Cats',\n",
              " 'dogs',\n",
              " 'horses',\n",
              " 'rabbits',\n",
              " 'birds',\n",
              " 'fish',\n",
              " 'turtles',\n",
              " 'hamsters',\n",
              " 'guinea pigs',\n",
              " 'ferrets',\n",
              " 'and many more.',\n",
              " 'Dogs have been human companions for thousands of years',\n",
              " 'with evidence of dog domestication dating back to around 15',\n",
              " '000 years ago. They were initially bred for tasks such as hunting',\n",
              " 'herding',\n",
              " 'and guarding',\n",
              " 'but today they are often kept as pets due to their affectionate and loyal nature.',\n",
              " 'Some popular breeds of dogs include:',\n",
              " 'Labrador retrievers',\n",
              " 'German shepherds',\n",
              " 'golden retrievers',\n",
              " 'French bulldogs',\n",
              " 'poodles',\n",
              " 'beagles',\n",
              " 'boxers',\n",
              " 'and bulldogs.',\n",
              " 'Dogs come in a wide range of sizes',\n",
              " 'from the tiny Chihuahua to the massive Great Dane. Each breed has its unique characteristics',\n",
              " 'temperament',\n",
              " 'and needs',\n",
              " 'making it essential to research and understand the specific requirements of a particular breed before bringing one home.',\n",
              " 'Dogs are highly social animals that thrive on interaction and attention from their owners. They require regular exercise',\n",
              " 'training',\n",
              " 'and veterinary care to stay happy and healthy.',\n",
              " 'Some fun facts about dogs include:',\n",
              " 'They can be trained to perform a wide range of tasks',\n",
              " 'from simple tricks to complex commands.',\n",
              " 'Dogs have a powerful sense of smell and can detect a wide range of scents.',\n",
              " 'Dogs are highly intelligent and can learn to communicate with humans through body language and vocalizations.',\n",
              " 'Dogs can be prone to certain health issues',\n",
              " 'such as hip dysplasia and eye problems',\n",
              " 'which require regular veterinary care.',\n",
              " 'Overall',\n",
              " \"dogs are incredible animals that bring immense joy and companionship to our lives. Whether you're a seasoned dog owner or just thinking of getting one\",\n",
              " \"there's never been a better time to welcome a furry friend into your family.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "pT06r_SffMfr",
        "outputId": "c8e9c65d-d59d-4d24-85ae-c8f0fd2395d9"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWrite a blog post about dogs\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n**The Joy of Dogs: A Comprehensive Guide**\\n\\nDogs are one of the most beloved pets in the world, and for good reason. They bring immense joy, companionship, and love to our lives. Here's a list of some interesting facts about dogs:\\n\\nCats, dogs, horses, rabbits, birds, fish, turtles, hamsters, guinea pigs, ferrets, and many more.\\n\\nDogs have been human companions for thousands of years, with evidence of dog domestication dating back to around 15,000 years ago. They were initially bred for tasks such as hunting, herding, and guarding, but today they are often kept as pets due to their affectionate and loyal nature.\\n\\nSome popular breeds of dogs include:\\n\\nLabrador retrievers, German shepherds, golden retrievers, French bulldogs, poodles, beagles, boxers, and bulldogs.\\n\\nDogs come in a wide range of sizes, from the tiny Chihuahua to the massive Great Dane. Each breed has its unique characteristics, temperament, and needs, making it essential to research and understand the specific requirements of a particular breed before bringing one home.\\n\\nDogs are highly social animals that thrive on interaction and attention from their owners. They require regular exercise, training, and veterinary care to stay happy and healthy.\\n\\nSome fun facts about dogs include:\\n\\nThey can be trained to perform a wide range of tasks, from simple tricks to complex commands.\\nDogs have a powerful sense of smell and can detect a wide range of scents.\\nDogs are highly intelligent and can learn to communicate with humans through body language and vocalizations.\\nDogs can be prone to certain health issues, such as hip dysplasia and eye problems, which require regular veterinary care.\\n\\nOverall, dogs are incredible animals that bring immense joy and companionship to our lives. Whether you're a seasoned dog owner or just thinking of getting one, there's never been a better time to welcome a furry friend into your family.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Parsing Output- Part Two"
      ],
      "metadata": {
        "id": "Lg1wU6FClgFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import DatetimeOutputParser\n",
        "\n",
        "output_parser= DatetimeOutputParser()"
      ],
      "metadata": {
        "id": "DpXh2l7Gfb5U"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.get_format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-tCVirrrlvAG",
        "outputId": "4aeadd7d-0eef-40ae-feb6-26f44f6a7b09"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 477-05-03T00:40:44.227402Z, 903-06-30T05:00:50.482273Z, 439-03-25T16:43:47.225071Z\\n\\nReturn ONLY this string, no other words!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_text= \"{request}\\n{format_instructions}\"\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(template_text)"
      ],
      "metadata": {
        "id": "AdEIqA8jl1at"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_prompt.get_format_instructions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmerigS3mIqm",
        "outputId": "ed2ea322-7bac-4616-dbbc-82de7fb2116e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['format_instructions', 'request']"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_request= chat_prompt.format_prompt(request= \"What date was the 13th Amendment ratified in the US?\",\n",
        "                          format_instructions= output_parser.get_format_instructions()).to_messages()\n",
        "\n",
        "result= model(model_request, temperature=0)\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "5E5AUOq0mQpi",
        "outputId": "f29037e9-3cc3-4f7f-d276-1bc53fc3e726"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 16-04-13T00:59:57.741818Z, 1930-05-16T02:10:23.793861Z, 1879-03-14T21:08:07.670341Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2021-01-01T00:00:00.000000Z\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.parse(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "E322a24wnSb7",
        "outputId": "8f27b571-49af-4d68-9faf-b17112f5115c"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat date was the 13th Amendment ratified in the US?\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 16-04-13T00:59:57.741818Z, 1930-05-16T02:10:23.793861Z, 1879-03-14T21:08:07.670341Z\n\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2021-01-01T00:00:00.000000Z\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/datetime.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    567\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n",
            "\u001b[0;31mValueError\u001b[0m: time data \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 16-04-13T00:59:57.741818Z, 1930-05-16T02:10:23.793861Z, 1879-03-14T21:08:07.670341Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2021-01-01T00:00:00.000000Z\" does not match format '%Y-%m-%dT%H:%M:%S.%fZ'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-cbe1134b9dc6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/datetime.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             raise OutputParserException(\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;34mf\"Could not parse datetime string: {response}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             ) from e\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat date was the 13th Amendment ratified in the US?\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 16-04-13T00:59:57.741818Z, 1930-05-16T02:10:23.793861Z, 1879-03-14T21:08:07.670341Z\n\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2021-01-01T00:00:00.000000Z\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_text= \"{request}\\n{format_instructions}\"\n",
        "human_prompt = HumanMessagePromptTemplate.from_template(template_text)\n",
        "\n",
        "system_prompt= SystemMessagePromptTemplate.from_template(\"You always reply to questions only in datetime patterns.\")\n",
        "\n",
        "chat_prompt= ChatPromptTemplate.from_messages([system_prompt, human_prompt])\n",
        "\n",
        "model_request= chat_prompt.format_prompt(request= \"What date was the 13th Amendment ratified in the US?\",\n",
        "                          format_instructions= output_parser.get_format_instructions()).to_messages()\n",
        "\n",
        "result= model(model_request, temperature=0)\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BUx3BkuxnSkB",
        "outputId": "43da1106-5d28-4f86-ff47-fa4d33515527"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2022-01-01T00:00:00.000000Z\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cmL4DQenSr3",
        "outputId": "05d89423-dacc-42c3-85e3-ceaad295f470"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "You always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What date was the 13th Amendment ratified in the US?\n",
            "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
            "\n",
            "Examples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\n",
            "\n",
            "Return ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "2022-01-01T00:00:00.000000Z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import OutputFixingParser\n",
        "\n",
        "misformatted= result.content\n",
        "misformatted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "KymATOHQmpKv",
        "outputId": "8f0dbb62-2f6a-4c63-cf65-805892adcbaa"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2022-01-01T00:00:00.000000Z\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_parser= OutputFixingParser.from_llm(parser= output_parser, llm= model)\n",
        "new_parser.parse(misformatted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4GweNZdYpREl",
        "outputId": "732ba6bf-d050-4178-ff7e-2d07df2f603f"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutputParserException",
          "evalue": "Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nInstructions:\n--------------\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 1841-10-20T16:59:33.176811Z, 1676-09-01T03:11:38.845085Z, 688-09-12T05:58:09.848759Z\n\nReturn ONLY this string, no other words!\n--------------\nCompletion:\n--------------\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat date was the 13th Amendment ratified in the US?\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\n\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2022-01-01T00:00:00.000000Z\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\nOutputParserException(\"Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2022-01-01T00:00:00.000000Z\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \")\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2022-01-01T00:00:00.000000Z\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/datetime.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    567\u001b[0m     format string.\"\"\"\n\u001b[0;32m--> 568\u001b[0;31m     \u001b[0mtt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         raise ValueError(\"time data %r does not match format %r\" %\n\u001b[0m\u001b[1;32m    350\u001b[0m                          (data_string, format))\n",
            "\u001b[0;31mValueError\u001b[0m: time data '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nInstructions:\\n--------------\\nWrite a datetime string that matches the following pattern: \\'%Y-%m-%dT%H:%M:%S.%fZ\\'.\\n\\nExamples: 1841-10-20T16:59:33.176811Z, 1676-09-01T03:11:38.845085Z, 688-09-12T05:58:09.848759Z\\n\\nReturn ONLY this string, no other words!\\n--------------\\nCompletion:\\n--------------\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: \\'%Y-%m-%dT%H:%M:%S.%fZ\\'.\\n\\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2022-01-01T00:00:00.000000Z\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\nOutputParserException(\"Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\nCutting Knowledge Date: December 2023\\\\nToday Date: 07 Dec 2024\\\\n\\\\nYou always reply to questions only in datetime patter...",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-b7e2775edb03>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_parser\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mOutputFixingParser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moutput_parser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmisformatted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/fix.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, completion)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOutputParserException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mretries\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/fix.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, completion)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOutputParserException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/output_parsers/datetime.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             raise OutputParserException(\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0;34mf\"Could not parse datetime string: {response}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             ) from e\n",
            "\u001b[0;31mOutputParserException\u001b[0m: Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nInstructions:\n--------------\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 1841-10-20T16:59:33.176811Z, 1676-09-01T03:11:38.845085Z, 688-09-12T05:58:09.848759Z\n\nReturn ONLY this string, no other words!\n--------------\nCompletion:\n--------------\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 07 Dec 2024\n\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat date was the 13th Amendment ratified in the US?\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\n\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2022-01-01T00:00:00.000000Z\n--------------\n\nAbove, the Completion did not satisfy the constraints given in the Instructions.\nError:\n--------------\nOutputParserException(\"Could not parse datetime string: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\nYou always reply to questions only in datetime patterns.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat date was the 13th Amendment ratified in the US?\\nWrite a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 796-02-10T14:41:29.717010Z, 679-10-09T01:54:49.506275Z, 800-02-12T10:13:19.710320Z\\n\\nReturn ONLY this string, no other words!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n2022-01-01T00:00:00.000000Z\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \")\n--------------\n\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n2022-01-01T00:00:00.000000Z\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Parsing Output- Part Three"
      ],
      "metadata": {
        "id": "W-D7a7Fap8ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "p5Pm1mvSpscG"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scientist(BaseModel):\n",
        "  name: str= Field(description= \"Name of Scientist\")\n",
        "  discoveries: list= Field(description= \"Python list of discoveries\")"
      ],
      "metadata": {
        "id": "Pggs6YOop_r_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser= PydanticOutputParser(pydantic_object= Scientist)\n",
        "print(parser.get_format_instructions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtbP55oyqfKg",
        "outputId": "12371461-e4fb-4ad4-c165-260ba9a3ea57"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"description\": \"Name of Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "human_prompt= HumanMessagePromptTemplate.from_template(\"{request}\\n{format_instructions}\")\n",
        "chat_prompt= ChatPromptTemplate.from_messages([human_prompt])\n",
        "\n",
        "request= chat_prompt.format_prompt(request= \"Tell me about the famous scientist\",\n",
        "                                  format_instructions= parser.get_format_instructions()).to_messages()\n",
        "\n",
        "result= model(request, temperature=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaC9Wuc3q2-Z",
        "outputId": "a5077fbf-4ebd-4cf0-cee2-6bd3172434cb"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "BlggRrTRri_o",
        "outputId": "5c23e55b-0463-45a6-820f-86bc96816619"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 07 Dec 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nTell me about the famous scientist\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"Name of Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\\n```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nHere\\'s an example of a JSON object that conforms to the provided schema:\\n\\n```json\\n{\\n  \"name\": \"Albert Einstein\",\\n  \"discoveries\": [\\n    \"Theory of Relativity\",\\n    \"Photons and Brownian motion\",\\n    \"Unified Field Theory\"\\n  ]\\n}\\n```\\n\\nThis object has the required properties (`name` and `discoveries`) and conforms to the specified structure.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5yYshjarm8c",
        "outputId": "368fcbbd-7c09-405b-f940-d3ef3366152a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 07 Dec 2024\n",
            "\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell me about the famous scientist\n",
            "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
            "\n",
            "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
            "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
            "\n",
            "Here is the output schema:\n",
            "```\n",
            "{\"properties\": {\"name\": {\"description\": \"Name of Scientist\", \"title\": \"Name\", \"type\": \"string\"}, \"discoveries\": {\"description\": \"Python list of discoveries\", \"items\": {}, \"title\": \"Discoveries\", \"type\": \"array\"}}, \"required\": [\"name\", \"discoveries\"]}\n",
            "```<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here's an example of a JSON object that conforms to the provided schema:\n",
            "\n",
            "```json\n",
            "{\n",
            "  \"name\": \"Albert Einstein\",\n",
            "  \"discoveries\": [\n",
            "    \"Theory of Relativity\",\n",
            "    \"Photons and Brownian motion\",\n",
            "    \"Unified Field Theory\"\n",
            "  ]\n",
            "}\n",
            "```\n",
            "\n",
            "This object has the required properties (`name` and `discoveries`) and conforms to the specified structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.9 Serialization- Saving and Loading Prompts"
      ],
      "metadata": {
        "id": "Vowi33UPsEUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"Tell me about a fact about {planet}\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"planet\"])\n",
        "\n",
        "# Save the prompt to a file\n",
        "prompt.save(\"myprompt.json\")"
      ],
      "metadata": {
        "id": "L94HXtxJr_Ik"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import load_prompt\n",
        "\n",
        "loaded_prompt= load_prompt(\"myprompt.json\")\n",
        "loaded_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DSSHctWsP27",
        "outputId": "11991879-9164-4161-f59a-b68f44723495"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['planet'], input_types={}, partial_variables={}, template='Tell me about a fact about {planet}')"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_prompt.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tega_IhyvAQ0",
        "outputId": "4157cb28-cba3-4dd0-eb5e-0116b373f15c"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['planet']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MKrBpX9qvCkN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}